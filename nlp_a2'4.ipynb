{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Roll no: 2403a52009\n",
        "\n",
        "batch: 2"
      ],
      "metadata": {
        "id": "6e_g3vvf_zb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wd5ShHs368OC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a579b1a6"
      },
      "source": [
        "# Task\n",
        "Analyze a provided medical text by applying NLTK and spaCy for sentence and word tokenization, then perform stemming using NLTK's Porter Stemmer and lemmatization using both NLTK and spaCy, finally comparing the outputs of stemming and lemmatization to identify their differences and assess the importance of lemmatization in preserving meaning for medical terminology."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aa30be6"
      },
      "source": [
        "## Load Medical Text\n",
        "\n",
        "### Subtask:\n",
        "Define the provided medical text as the input corpus for processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fda0c81"
      },
      "source": [
        "**Reasoning**:\n",
        "To define the medical text as the input corpus, I will create a Python variable `medical_text` and assign a meaningful medical passage to it as a string.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "888cbe75",
        "outputId": "4245128c-7d10-4520-c802-b0f7276e06e4"
      },
      "source": [
        "medical_text = \"\"\"The human body is a complex biological system comprising cells, tissues, organs, and organ systems that work together to maintain homeostasis. Key systems include the circulatory system (heart, blood vessels, blood) which transports oxygen and nutrients, the respiratory system (lungs, airways) for gas exchange, and the nervous system (brain, spinal cord, nerves) which controls and coordinates body functions. The digestive system processes food, while the endocrine system regulates hormones. Understanding these intricate interactions is fundamental to medicine and healthcare.\"\"\"\n",
        "print(\"Medical text loaded into 'medical_text' variable.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical text loaded into 'medical_text' variable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff98d64a"
      },
      "source": [
        "## Sentence Tokenization\n",
        "\n",
        "### Subtask:\n",
        "Apply NLTK's sentence tokenizer to split the medical text into individual sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee608435",
        "outputId": "a4398c7b-df2b-43b5-852e-7f9fc11dc9f0"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download the 'punkt' tokenizer data if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Apply NLTK's sentence tokenizer\n",
        "sentences = sent_tokenize(medical_text)\n",
        "\n",
        "# Print the tokenized sentences\n",
        "print(\"Tokenized sentences using NLTK:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentences using NLTK:\n",
            "Sentence 1: The human body is a complex biological system comprising cells, tissues, organs, and organ systems that work together to maintain homeostasis.\n",
            "Sentence 2: Key systems include the circulatory system (heart, blood vessels, blood) which transports oxygen and nutrients, the respiratory system (lungs, airways) for gas exchange, and the nervous system (brain, spinal cord, nerves) which controls and coordinates body functions.\n",
            "Sentence 3: The digestive system processes food, while the endocrine system regulates hormones.\n",
            "Sentence 4: Understanding these intricate interactions is fundamental to medicine and healthcare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e05a8367",
        "outputId": "8daa5988-3377-47d9-a1e3-82a139d79018"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download the 'punkt' tokenizer data if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Apply NLTK's sentence tokenizer\n",
        "sentences = sent_tokenize(medical_text)\n",
        "\n",
        "# Print the tokenized sentences\n",
        "print(\"Tokenized sentences using NLTK:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentences using NLTK:\n",
            "Sentence 1: The human body is a complex biological system comprising cells, tissues, organs, and organ systems that work together to maintain homeostasis.\n",
            "Sentence 2: Key systems include the circulatory system (heart, blood vessels, blood) which transports oxygen and nutrients, the respiratory system (lungs, airways) for gas exchange, and the nervous system (brain, spinal cord, nerves) which controls and coordinates body functions.\n",
            "Sentence 3: The digestive system processes food, while the endocrine system regulates hormones.\n",
            "Sentence 4: Understanding these intricate interactions is fundamental to medicine and healthcare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d68a61bd",
        "outputId": "7b684a9b-79b7-445f-9241-d70afc4d0e75"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download the 'punkt' tokenizer data if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Download the 'punkt_tab' resource if not already downloaded, as it's required by sent_tokenize\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "# Apply NLTK's sentence tokenizer\n",
        "sentences = sent_tokenize(medical_text)\n",
        "\n",
        "# Print the tokenized sentences\n",
        "print(\"Tokenized sentences using NLTK:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentences using NLTK:\n",
            "Sentence 1: The human body is a complex biological system comprising cells, tissues, organs, and organ systems that work together to maintain homeostasis.\n",
            "Sentence 2: Key systems include the circulatory system (heart, blood vessels, blood) which transports oxygen and nutrients, the respiratory system (lungs, airways) for gas exchange, and the nervous system (brain, spinal cord, nerves) which controls and coordinates body functions.\n",
            "Sentence 3: The digestive system processes food, while the endocrine system regulates hormones.\n",
            "Sentence 4: Understanding these intricate interactions is fundamental to medicine and healthcare.\n",
            "Tokenized sentences using NLTK:\n",
            "Sentence 1: The human body is a complex biological system comprising cells, tissues, organs, and organ systems that work together to maintain homeostasis.\n",
            "Sentence 2: Key systems include the circulatory system (heart, blood vessels, blood) which transports oxygen and nutrients, the respiratory system (lungs, airways) for gas exchange, and the nervous system (brain, spinal cord, nerves) which controls and coordinates body functions.\n",
            "Sentence 3: The digestive system processes food, while the endocrine system regulates hormones.\n",
            "Sentence 4: Understanding these intricate interactions is fundamental to medicine and healthcare.\n",
            "Tokenized sentences using NLTK:\n",
            "Sentence 1: The human body is a complex biological system comprising cells, tissues, organs, and organ systems that work together to maintain homeostasis.\n",
            "Sentence 2: Key systems include the circulatory system (heart, blood vessels, blood) which transports oxygen and nutrients, the respiratory system (lungs, airways) for gas exchange, and the nervous system (brain, spinal cord, nerves) which controls and coordinates body functions.\n",
            "Sentence 3: The digestive system processes food, while the endocrine system regulates hormones.\n",
            "Sentence 4: Understanding these intricate interactions is fundamental to medicine and healthcare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed3477ce"
      },
      "source": [
        "## Word Tokenization\n",
        "\n",
        "### Subtask:\n",
        "Apply NLTK's word tokenizer to split each sentence into individual words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3dfe2a3"
      },
      "source": [
        "## Word Tokenization\n",
        "\n",
        "### Subtask:\n",
        "Apply NLTK's word tokenizer to split each sentence into individual words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a416572"
      },
      "source": [
        "**Reasoning**:\n",
        "To apply NLTK's word tokenizer, I will import the `word_tokenize` function, ensure the 'punkt' tokenizer data is available, iterate through the previously tokenized sentences, apply word tokenization to each, and collect all words into a list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9acd57a1",
        "outputId": "15dd006d-2ee4-43ba-9370-c942bfbb5ed6"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure 'punkt' tokenizer data is downloaded for word tokenization as well\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Create an empty list to store all tokenized words\n",
        "nltk_words = []\n",
        "\n",
        "# Iterate through each sentence and apply word tokenization\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    nltk_words.extend(words)\n",
        "\n",
        "# Print the first few tokenized words\n",
        "print(\"First 20 tokenized words using NLTK:\")\n",
        "print(nltk_words[:20])\n",
        "print(f\"Total words tokenized by NLTK: {len(nltk_words)}\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 tokenized words using NLTK:\n",
            "['The', 'human', 'body', 'is', 'a', 'complex', 'biological', 'system', 'comprising', 'cells', ',', 'tissues', ',', 'organs', ',', 'and', 'organ', 'systems', 'that', 'work']\n",
            "Total words tokenized by NLTK: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2af90661"
      },
      "source": [
        "## Word Tokenization\n",
        "\n",
        "### Subtask:\n",
        "Perform word tokenization using spaCy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf808667"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform word tokenization using spaCy, I will first ensure the 'en_core_web_sm' model is downloaded and loaded, then process the `medical_text` to extract and store the tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00c33b5d",
        "outputId": "50393fb1-d648-4059-b119-bd05e2b56a5c"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading 'en_core_web_sm' spaCy model...\")\n",
        "    spacy.cli.download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the medical text with spaCy\n",
        "doc = nlp(medical_text)\n",
        "\n",
        "# Extract tokens (words) from the processed spaCy document\n",
        "spacy_words = [token.text for token in doc]\n",
        "\n",
        "# Print the first few spaCy tokens and the total count\n",
        "print(\"First 20 tokenized words using spaCy:\")\n",
        "print(spacy_words[:20])\n",
        "print(f\"Total words tokenized by spaCy: {len(spacy_words)}\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 tokenized words using spaCy:\n",
            "['The', 'human', 'body', 'is', 'a', 'complex', 'biological', 'system', 'comprising', 'cells', ',', 'tissues', ',', 'organs', ',', 'and', 'organ', 'systems', 'that', 'work']\n",
            "Total words tokenized by spaCy: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b66a910"
      },
      "source": [
        "## Apply Stemming\n",
        "\n",
        "### Subtask:\n",
        "Apply the Porter Stemmer from NLTK to the words to get their root forms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06f09800"
      },
      "source": [
        "**Reasoning**:\n",
        "To apply Porter Stemming, I will import `PorterStemmer`, instantiate it, then iterate through `nltk_words` to stem each word and store the results in a new list, finally printing the first 20 stemmed words and their total count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5095ec4e",
        "outputId": "05f283d3-702b-4dc9-e1c4-27e94e129811"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Instantiate Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Create an empty list to store stemmed words\n",
        "stemmed_words = []\n",
        "\n",
        "# Iterate through the nltk_words and apply stemming\n",
        "for word in nltk_words:\n",
        "    stemmed_words.append(porter_stemmer.stem(word))\n",
        "\n",
        "# Print the first 20 stemmed words and the total count\n",
        "print(\"First 20 stemmed words using NLTK Porter Stemmer:\")\n",
        "print(stemmed_words[:20])\n",
        "print(f\"Total stemmed words: {len(stemmed_words)}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 stemmed words using NLTK Porter Stemmer:\n",
            "['the', 'human', 'bodi', 'is', 'a', 'complex', 'biolog', 'system', 'compris', 'cell', ',', 'tissu', ',', 'organ', ',', 'and', 'organ', 'system', 'that', 'work']\n",
            "Total stemmed words: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "413f5f1c"
      },
      "source": [
        "## Apply Lemmatization (NLTK)\n",
        "\n",
        "### Subtask:\n",
        "Apply NLTK's WordNetLemmatizer to the words to get their base forms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdde7246",
        "outputId": "4af69700-a93e-44ee-d31d-0fd07ee1588c"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download 'wordnet' corpus if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "# Instantiate WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Create an empty list to store lemmatized words\n",
        "nltk_lemmas = []\n",
        "\n",
        "# Iterate through the nltk_words and apply lemmatization\n",
        "# For simplicity, we are assuming a default part-of-speech (noun) for lemmatization.\n",
        "# More advanced lemmatization would require POS tagging first.\n",
        "for word in nltk_words:\n",
        "    nltk_lemmas.append(wordnet_lemmatizer.lemmatize(word))\n",
        "\n",
        "# Print the first 20 lemmatized words and the total count\n",
        "print(\"First 20 lemmatized words using NLTK WordNetLemmatizer:\")\n",
        "print(nltk_lemmas[:20])\n",
        "print(f\"Total lemmatized words by NLTK: {len(nltk_lemmas)}\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 lemmatized words using NLTK WordNetLemmatizer:\n",
            "['The', 'human', 'body', 'is', 'a', 'complex', 'biological', 'system', 'comprising', 'cell', ',', 'tissue', ',', 'organ', ',', 'and', 'organ', 'system', 'that', 'work']\n",
            "Total lemmatized words by NLTK: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "293e993a"
      },
      "source": [
        "## Apply Lemmatization\n",
        "\n",
        "### Subtask:\n",
        "Apply spaCy for lemmatization to get the base forms of the words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e47b7a9"
      },
      "source": [
        "**Reasoning**:\n",
        "To apply spaCy for lemmatization, I will iterate through the previously processed spaCy document (`doc`), extract the lemma for each token, store them in a list, and then print the first 20 lemmas and their total count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d9a3d62",
        "outputId": "c8d52699-76cd-4bc2-bbc0-88424b0adcd9"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# The nlp model and doc object should already be loaded/created from previous steps\n",
        "# If not, ensure these steps are executed:\n",
        "# try:\n",
        "#     nlp = spacy.load(\"en_core_web_sm\")\n",
        "# except OSError:\n",
        "#     spacy.cli.download(\"en_core_web_sm\")\n",
        "#     nlp = spacy.load(\"en_core_web_sm\")\n",
        "# doc = nlp(medical_text)\n",
        "\n",
        "# Create an empty list to store lemmatized words\n",
        "spacy_lemmas = []\n",
        "\n",
        "# Iterate through the spaCy doc and extract lemmas\n",
        "for token in doc:\n",
        "    spacy_lemmas.append(token.lemma_)\n",
        "\n",
        "# Print the first 20 lemmatized words and the total count\n",
        "print(\"First 20 lemmatized words using spaCy:\")\n",
        "print(spacy_lemmas[:20])\n",
        "print(f\"Total lemmatized words by spaCy: {len(spacy_lemmas)}\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 lemmatized words using spaCy:\n",
            "['the', 'human', 'body', 'be', 'a', 'complex', 'biological', 'system', 'comprise', 'cell', ',', 'tissue', ',', 'organ', ',', 'and', 'organ', 'system', 'that', 'work']\n",
            "Total lemmatized words by spaCy: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed63c196"
      },
      "source": [
        "## Compare Stemming and Lemmatization\n",
        "\n",
        "### Subtask:\n",
        "Display and compare the original words, their stemmed versions, and their lemmatized forms to highlight the differences, especially in a medical context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "011c6bab"
      },
      "source": [
        "**Reasoning**:\n",
        "To display and compare the original words, their stemmed versions, and their lemmatized forms, I will iterate through the previously generated lists (`nltk_words`, `stemmed_words`, `nltk_lemmas`, `spacy_lemmas`) using `zip` to combine them into tuples, then print a formatted table of the first 30 entries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c298ab42",
        "outputId": "afab440b-e41c-467f-ee3f-31979b97c0c4"
      },
      "source": [
        "comparison_data = []\n",
        "\n",
        "# Populate the comparison data list\n",
        "for word, stemmed, nltk_lemma, spacy_lemma in zip(nltk_words, stemmed_words, nltk_lemmas, spacy_lemmas):\n",
        "    comparison_data.append((word, stemmed, nltk_lemma, spacy_lemma))\n",
        "\n",
        "# Print header\n",
        "print(f\"{\"Original Word\":<18} | {\"NLTK Stem\":<15} | {\"NLTK Lemma\":<15} | {\"spaCy Lemma\":<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Print a representative sample (e.g., first 30 words)\n",
        "for i, (word, stemmed, nltk_lemma, spacy_lemma) in enumerate(comparison_data[:30]):\n",
        "    print(f\"{word:<18} | {stemmed:<15} | {nltk_lemma:<15} | {spacy_lemma:<15}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Word      | NLTK Stem       | NLTK Lemma      | spaCy Lemma    \n",
            "----------------------------------------------------------------------\n",
            "The                | the             | The             | the            \n",
            "human              | human           | human           | human          \n",
            "body               | bodi            | body            | body           \n",
            "is                 | is              | is              | be             \n",
            "a                  | a               | a               | a              \n",
            "complex            | complex         | complex         | complex        \n",
            "biological         | biolog          | biological      | biological     \n",
            "system             | system          | system          | system         \n",
            "comprising         | compris         | comprising      | comprise       \n",
            "cells              | cell            | cell            | cell           \n",
            ",                  | ,               | ,               | ,              \n",
            "tissues            | tissu           | tissue          | tissue         \n",
            ",                  | ,               | ,               | ,              \n",
            "organs             | organ           | organ           | organ          \n",
            ",                  | ,               | ,               | ,              \n",
            "and                | and             | and             | and            \n",
            "organ              | organ           | organ           | organ          \n",
            "systems            | system          | system          | system         \n",
            "that               | that            | that            | that           \n",
            "work               | work            | work            | work           \n",
            "together           | togeth          | together        | together       \n",
            "to                 | to              | to              | to             \n",
            "maintain           | maintain        | maintain        | maintain       \n",
            "homeostasis        | homeostasi      | homeostasis     | homeostasis    \n",
            ".                  | .               | .               | .              \n",
            "Key                | key             | Key             | key            \n",
            "systems            | system          | system          | system         \n",
            "include            | includ          | include         | include        \n",
            "the                | the             | the             | the            \n",
            "circulatory        | circulatori     | circulatory     | circulatory    \n"
          ]
        }
      ]
    }
  ]
}